// Copyright 2022 Lucas Javaudin
//
// Licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International
// https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode

//! Everything related to simulation parameters.
use schemars::JsonSchema;
use serde_derive::{Deserialize, Serialize};
use ttf::TTFNum;

use crate::learning::LearningModel;
use crate::network::{NetworkParameters, NetworkWeights};
use crate::simulation::results::AgentResults;
use crate::stop::StopCriterion;
use crate::units::Interval;

const fn default_update_ratio() -> f64 {
    1.0
}

/// Set of parameters used to control how a [Simulation](crate::simulation::Simulation) is run.
#[derive(Clone, Debug, Deserialize, Serialize, JsonSchema)]
#[serde(bound(deserialize = "T: TTFNum"))]
#[schemars(example = "crate::schema::example_parameters")]
pub struct Parameters<T> {
    /// Time interval used to restrict the travel-time functions of the edges.
    ///
    /// The departure-time intervals of the agents must be included in this interval.
    ///
    /// Agents can still travel on the network when the period is exceeded but the edges' travel
    /// times are no longer recorded.
    /// The departure time chosen by any agent must be such that the expected arrival time is
    /// earlier than the end of the period.
    pub period: Interval<T>,
    /// Set of parameters for the network.
    pub network: NetworkParameters<T>,
    /// Learning model used to update the values between two iterations.
    #[serde(default)]
    pub learning_model: LearningModel<T>,
    /// Set of stopping criteria used to decide when the iterative process should stop.
    pub stopping_criteria: Vec<StopCriterion<T>>,
    /// Share of agents that can update their pre-day choices at each iteration.
    #[serde(default = "default_update_ratio")]
    #[validate(range(min = 0.0, max = 1.0))]
    pub update_ratio: f64,
    /// Random seed used for all the draws.
    ///
    /// If `null`, the seed is generated by entropy.
    #[serde(default)]
    pub random_seed: Option<u64>,
}

impl<T> Parameters<T> {
    /// Creates a new set of parameters.
    pub fn new(
        period: Interval<T>,
        network: NetworkParameters<T>,
        learning_model: LearningModel<T>,
        stopping_criteria: Vec<StopCriterion<T>>,
        update_ratio: f64,
        random_seed: Option<u64>,
    ) -> Self {
        Parameters {
            period,
            network,
            learning_model,
            stopping_criteria,
            update_ratio,
            random_seed,
        }
    }
}

impl<T: TTFNum> Parameters<T> {
    /// Returns `true` if the Simulation must be stopped.
    ///
    /// The Simulation is stopped if at least one of the stopping criteria is active.
    pub fn stop(
        &self,
        iteration_counter: u32,
        results: &AgentResults<T>,
        prev_results: Option<&AgentResults<T>>,
    ) -> bool {
        self.stopping_criteria
            .iter()
            .any(|c| c.stop(iteration_counter, results, prev_results))
    }

    /// Returns the new [NetworkWeights] given the old weights and the simulated weights.
    pub fn learn(
        &self,
        old_weights: &NetworkWeights<T>,
        weights: &NetworkWeights<T>,
        iteration_counter: u32,
    ) -> NetworkWeights<T> {
        // At this point, the iteration counter has not been increment yet.
        let mut new_weights =
            self.learning_model
                .learn(old_weights, weights, iteration_counter + 1);
        new_weights.simplify(&self.network);
        new_weights
    }
}
