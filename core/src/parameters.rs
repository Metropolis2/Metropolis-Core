// Copyright 2022 Lucas Javaudin
//
// Licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International
// https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode

//! Everything related to simulation parameters.
use std::path::PathBuf;

use schemars::JsonSchema;
use serde_derive::{Deserialize, Serialize};
use ttf::TTFNum;

use crate::learning::LearningModel;
use crate::network::road_network::RoadNetworkParameters;
use crate::network::NetworkWeights;
use crate::simulation::results::AgentResults;
use crate::units::Interval;

const fn default_iteration_counter() -> u32 {
    1
}

const fn default_update_ratio() -> f64 {
    1.0
}

/// Format to be used when saving files.
#[derive(Clone, Copy, Debug, Default, Deserialize, Serialize, JsonSchema)]
pub enum SavingFormat {
    /// Zstd-compressed JSON files.
    JSON,
    /// Parquet files.
    #[default]
    Parquet,
    /// CSV files.
    CSV,
}

/// Struct to store all the input file paths.
#[derive(Clone, Debug, Default, Deserialize, Serialize, JsonSchema)]
pub struct InputFiles {
    pub(crate) agents: PathBuf,
    pub(crate) alternatives: PathBuf,
    #[serde(default)]
    pub(crate) trips: Option<PathBuf>,
    #[serde(default)]
    pub(crate) edges: Option<PathBuf>,
    #[serde(default)]
    pub(crate) vehicle_types: Option<PathBuf>,
    #[serde(default)]
    pub(crate) road_network_conditions: Option<PathBuf>,
}

/// Set of parameters used to control how a [Simulation](crate::simulation::Simulation) is run.
#[derive(Clone, Debug, Deserialize, Serialize, JsonSchema)]
#[serde(bound(deserialize = "T: TTFNum"))]
#[schemars(example = "crate::schema::example_parameters")]
pub struct Parameters<T> {
    /// Paths to the input files.
    pub input_files: InputFiles,
    /// Path to the output directory.
    #[serde(default)]
    pub output_directory: PathBuf,
    /// Time interval used to restrict the travel-time functions of the edges.
    ///
    /// The departure-time intervals of the agents must be included in this interval.
    ///
    /// Agents can still travel on the network when the period is exceeded but the edges' travel
    /// times are no longer recorded.
    /// The departure time chosen by any agent must be such that the expected arrival time is
    /// earlier than the end of the period.
    pub period: Interval<T>,
    /// Initial iteration counter to use for the simulation.
    ///
    /// This is useful when running a simulation "step-by-step" (i.e., the input is modified
    /// partially from one iteration to another) so that, for example, the coefficients for the
    /// learning model are correctly computed.
    #[serde(default = "default_iteration_counter")]
    #[validate(range(min = 1))]
    pub init_iteration_counter: u32,
    /// Maximum number of iterations to be run (on top of the `init_iteration_counter`).
    pub max_iterations: u32,
    /// Set of parameters for the road network.
    pub road_network: Option<RoadNetworkParameters<T>>,
    /// Learning model used to update the values between two iterations.
    #[serde(default)]
    pub learning_model: LearningModel<T>,
    /// Share of agents that can update their pre-day choices at each iteration.
    #[serde(default = "default_update_ratio")]
    #[validate(range(min = 0.0, max = 1.0))]
    pub update_ratio: f64,
    /// Random seed used for all the draws.
    ///
    /// If `null`, the seed is generated by entropy.
    #[serde(default)]
    pub random_seed: Option<u64>,
    /// Number of threads to use for parallel tasks.
    ///
    /// Default (0) is to use all the threads of the CPU.
    #[serde(default)]
    pub nb_threads: usize,
    /// Format to use for saving the output files.
    #[serde(default)]
    pub saving_format: SavingFormat,
    /// If `true`, the simulation will compute the travel decisions for the first iteration, save
    /// them and stop immediately.
    #[serde(default)]
    pub only_compute_decisions: bool,
}

impl<T> Parameters<T> {
    pub(crate) fn saving_extension(&self) -> String {
        match self.saving_format {
            SavingFormat::JSON => "json".into(),
            SavingFormat::Parquet => "parquet".into(),
            SavingFormat::CSV => "csv".into(),
        }
    }
}

impl<T: TTFNum> Parameters<T> {
    /// Returns `true` if the Simulation must be stopped.
    pub fn stop(&self, iteration_counter: u32, _results: &AgentResults<T>) -> bool {
        debug_assert!(iteration_counter >= self.init_iteration_counter);
        let nb_iterations = 1 + iteration_counter - self.init_iteration_counter;
        nb_iterations >= self.max_iterations
    }

    /// Returns the new [NetworkWeights] given the old weights and the simulated weights.
    pub fn learn(
        &self,
        old_weights: &NetworkWeights<T>,
        weights: &NetworkWeights<T>,
        iteration_counter: u32,
    ) -> NetworkWeights<T> {
        // At this point, the iteration counter has not been increment yet.
        self.learning_model
            .learn(old_weights, weights, iteration_counter + 1)
    }
}
